# re-using from last ON week

import logging
import os

import certifi
import ecs_logging
import fire
import pandas as pd
from elasticsearch import Elasticsearch, helpers

# Get the Logger
logger = logging.getLogger("app")
logger.setLevel(logging.DEBUG)

# Add an ECS formatter to the Handler
handler = logging.StreamHandler()
handler.setFormatter(ecs_logging.StdlibFormatter())
logger.addHandler(handler)


class QueryMaker:
    def __init__(self):
        # todo: make parameters for all the pieces here
        self.es = Elasticsearch(
            hosts=[
                "https://394b04bd8a174e369434da90bf3701f4.us-east-1.aws.found.io:9243"
            ],
            api_key=(os.getenv("API_USER"), os.getenv("API_KEY")),
            ca_certs=certifi.where(),
        )
        self.scrolled_elasticsearch_query(
            index="alert_telemetry_elastic*",
            query={
                     # other options: reformat these fields in the index mapping, https://stackoverflow.com/questions/49326777/elasticsearch-source-filtering-object-array
                     # or do it afterwards on the results https://stackoverflow.com/questions/40871180/elasticsearch-py-search-using-source-filter
                    "_source":{
                        "includes": ["cloud.deployment_info.is_elastic_internal",
                                     "file.hash.sha256",
                                     "@timestamp",
                                     "Target.process.thread.Ext*",
                                     "Target.process.Ext.memory*",
                                     "data_stream.dataset",
                                     "process.hash.sha256",
                                     "process.Ext.memory_region.bytes_compressed"
                        ],
                        "excludes":["Endpoint.policy.applied.artifacts*", "_index", "_ignored", "_score", "_type"]
                    },
                          "query": {
                            "bool": {
                                "must":[
                                    {"match":
                                         {"cloud.deployment_info.is_elastic_internal": "true"}
                                     },
                                    {"match":
                                        {"process.Ext.memory_region.bytes_compressed_present": "true"}
                                 }]
                            }
                        }
            },
            max_results=50,
        )

    def scrolled_elasticsearch_query(
        self,
        index: str,
        query: dict,
        max_results: int = -1,
        query_size: int = 2000,
    ) -> None:
        """
        # simplified this from Ashton's notebook here: https://github.com/elastic/security-data-science/blob/ec1de4856c030d788b87028f0fc4bc35753c8942/demos/list_query_profiling.ipynb

        Use the scroll interface to Elasticsearch to run a query.

        note: this is painfully slow, scan is supposedly faster
        """
        values_to_return = []
        logger.info("Querying the analytics cluster", extra={"query": query})
        initial_results = self.es.search(
            index=index, body=query, scroll="15m", size=query_size, request_timeout=300
        )
        scroll_id: str = str(initial_results["_scroll_id"])
        finished: bool = False
        while not finished:
            scroll_results = self.es.scroll(scroll="15m", scroll_id=scroll_id)
            n_scroll_results = len(scroll_results["hits"]["hits"])

            if n_scroll_results > 0:
                values_to_return.extend([hit for hit in scroll_results["hits"]["hits"]])

            if n_scroll_results < query_size:
                finished = True

            if max_results > 0 and len(values_to_return) > max_results:
                values_to_return = values_to_return[:max_results]
                finished = True

        self.query_results = values_to_return

    def write_compressed_bytes(self, write_to_local:bool=True, write_to_s3:bool=True):
        """
        Takes the '_source.process.Ext.memory_region.bytes_compressed' field and writes it out for further processing
        Resulting filename
        # todo: make a new s3 bucket for these
        :param:
        :return:
        """
        # todo: have to de-duplicate, probably want to do that here unless there is a way to do it in the query

        for x in self.query_results:
            sha256 = x["_source"]['process']["hash"]['sha256']
            alert_id = x["_id"]
            timestamp = '-'.join(alert_id.split('-')[-3:])
            day = timestamp.split('T')[0]
            local_filename = f"{day}-{sha256}-{alert_id}"
            s3_filename = local_filename.replace('-', '/')
            bytes_field = x["process"]["Ext"]["memory_region"]["bytes_compressed"]

            # write local first:
            with open(f"/tmp/{local_filename}", "wb") as f:
                f.write(bytes_field)

            if write_to_s3:
                pass
                # todo: re-use existing code for this piece too

            if not write_to_local:
                # todo: remove from tmp
                pass
            else:
                # todo: could move to another folder or whatever since /tmp is uh, temporary
                pass
        pass


if __name__ == "__main__":
    fire.Fire(QueryMaker)