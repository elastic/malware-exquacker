# re-using from last ON week

import logging
import os

import certifi
import ecs_logging
import fire
import pandas as pd
from elasticsearch import Elasticsearch, helpers

# Get the Logger
logger = logging.getLogger("app")
logger.setLevel(logging.DEBUG)

# Add an ECS formatter to the Handler
handler = logging.StreamHandler()
handler.setFormatter(ecs_logging.StdlibFormatter())
logger.addHandler(handler)


class QueryMaker:
    def __init__(self):
        # todo: make parameters for all the pieces here
        self.es = Elasticsearch(
            hosts=[
                "https://394b04bd8a174e369434da90bf3701f4.us-east-1.aws.found.io:9243"
            ],
            api_key=(os.getenv("API_USER"), os.getenv("API_KEY")),
            use_ssl=True,
            ca_certs=certifi.where(),
        )
        self.scrolled_elasticsearch_query(
            index="alert_telemetry_elastic*",
            query={ "query":{
                      "bool": {
                        "must": [
                          {"match":
                           {"cloud.deployment_info.is_elastic_internal": "true",
                           "process.Ext.memory_region.bytes_compressed_present": "true"
                           }
                          }
                          ]
                      },
                          "docs": [
                                  {
                                    "_source":
                                    {"include": ["file.hash.sha256", "@timestamp", "Target.process.thread.Ext*", "Target.process.Ext.memory*", "data_stream.dataset", "process.Ext.memory_region.bytes_compressed"
                                    ],
                                      "exclude":["Endpoint.policy.applied.artifacts*"]
                                    }
                                  }]
                    }},
            max_results=50000,
        )

    def scrolled_elasticsearch_query(
        self,
        index: str,
        query: dict,
        max_results: int = -1,
        query_size: int = 2000,
    ) -> None:
        """
        # simplified this from Ashton's notebook here: https://github.com/elastic/security-data-science/blob/ec1de4856c030d788b87028f0fc4bc35753c8942/demos/list_query_profiling.ipynb

        Use the scroll interface to Elasticsearch to run a query.

        note: this is painfully slow, scan is supposedly faster
        """
        values_to_return = []
        logger.info("Querying the analytics cluster", extra={"query": query})
        initial_results = self.es.search(
            index=index, body=query, scroll="15m", size=query_size, request_timeout=300
        )
        scroll_id: str = str(initial_results["_scroll_id"])
        finished: bool = False
        while not finished:
            scroll_results = self.es.scroll(scroll="15m", scroll_id=scroll_id)
            n_scroll_results = len(scroll_results["hits"]["hits"])

            if n_scroll_results > 0:
                values_to_return.extend([hit for hit in scroll_results["hits"]["hits"]])

            if n_scroll_results < query_size:
                finished = True

            if max_results > 0 and len(values_to_return) > max_results:
                values_to_return = values_to_return[:max_results]
                finished = True

        self.query_results = values_to_return

    def make_dataframe(
        self,
        output_path: str = "",
        output_filename: str = "sample_data4",
    ) -> pd.DataFrame:
        """
        Helper function to convert the list of dicts to a dataframe, should we want that
        :param scrolled: list of dicts
        :param output_filename: doesn't matter that much for now, would be versioned with a timestamp IRL
        """
        scrolled = self.query_results
        sources = [x["_source"] for x in scrolled]
        scored = []
        for source in sources:
            ts = source["@timestamp"]
            tmp = source["model_scores"]
            sha256 = source["file"]["hash"]["sha256"]
            tmp.update({"hash": sha256, "ts": ts})
            scored.append(tmp)
        df = pd.DataFrame(scored)
        cols_to_keep = [x for x in df.columns if "latest" not in x]
        df = df[cols_to_keep]
        logger.info("Making the dataframe", extra={"df preview": df.head()})
        fullpath = os.path.join(output_path, output_filename)
        df.to_csv(fullpath)
        return df


if __name__ == "__main__":
    fire.Fire(QueryMaker)