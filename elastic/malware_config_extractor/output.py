from __future__ import annotations

from logging import getLogger
from queue import Empty
from typing import Iterable

from elasticsearch import Elasticsearch, helpers
from elasticsearch_dsl import connections
from scalpl import Cut

from elastic.malware_config_extractor.mappings import setup_index
from elastic.malware_config_extractor.process import POISON_PILL, OutputProcess
from elastic.malware_config_extractor.utils import connect_elasticsearch

logger = getLogger(__name__)


class ESOutput(OutputProcess):
    def __init__(self, *args, config: Cut, do_setup: bool = False, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        self.config: Cut = config
        self.es_client: Elasticsearch | None
        self.do_setup: bool = do_setup

    def _setup_io(self):
        logger.info("Setting up output ES connection")

        if self.config["enabled"] is True:
            logger.info("Connecting to Elasticsearch for output")
            self.es_client = connect_elasticsearch(dict(self.config))

            if self.es_client:
                logger.info("Successfully connected to Elasticsearch for output")
                connections.add_connection("output", self.es_client)

    def setup(self):
        """
        Installs index template and components
        """
        if not self.es_client:
            logger.error("Elasticsearch not connected for output setup!")
            raise ConnectionError("Elasticsearch not connected for output setup")

        try:
            setup_index(self.es_client, self.config["index"])
        except ValueError as error:
            raise error

    def load(self) -> Iterable:

        if self.do_setup is True:
            self.setup()

        _count: int = 0
        while True:
            if self.shutdown.is_set():
                break

            try:
                doc = self.inq.get_nowait()
            except Empty:
                continue

            if doc is POISON_PILL:
                self.inq.put(POISON_PILL)
                break
            else:
                _count += 1
                yield doc

        logger.info("Loaded %s documents", _count)

    def run(self) -> None:
        logger.debug("run")
        super().run()

        def chunk(chunk_size=500):
            block = []
            for entry in self.load():
                if len(block) > chunk_size:

                    yield block
                    block = [entry]
                else:
                    block.append(entry)

            if block:
                yield block

        # _es_docs, _con_docs = tee(self.load, 2)
        self._setup_io()

        if self.es_client:
            for _chunk in chunk():
                for _ok, _action in helpers.streaming_bulk(
                    client=self.es_client,
                    index=self.config["index"],
                    actions=_chunk,
                ):
                    pass

        logger.debug("Shutting down output")

        # Cleanup
        if self.es_client is not None:
            logger.debug("Closing Elasticsearch output client.")
            self.es_client.close()
