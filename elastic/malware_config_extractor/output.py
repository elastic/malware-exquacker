from logging import getLogger
from queue import Queue
from typing import Any, Generator

from elasticsearch import Elasticsearch, helpers
from elasticsearch_dsl import connections
from scalpl import Cut

from elastic.malware_config_extractor.mappings import setup_index
from elastic.malware_config_extractor.thread import BaseThread
from elastic.malware_config_extractor.utils import connect_elasticsearch, consume_queue

logger = getLogger(__name__)


class ESOutput(BaseThread):
    def __init__(
        self, config: Cut, taskq: Queue, do_setup: bool = False, **kwargs
    ) -> None:
        super().__init__(**kwargs)

        self.config: Cut = config
        self.es_client: Elasticsearch = None
        self.do_setup: bool = do_setup
        self.taskq: Queue = taskq

    def _setup_io(self):
        logger.info("Setting up output ES connection")

        if self.config["enabled"] is True:
            logger.info("Connecting to Elasticsearch for output")
            self.es_client = connect_elasticsearch(self.config)

            if self.es_client:
                logger.info("Successfully connected to Elasticsearch for output")
                connections.add_connection("output", self.es_client)

    def setup(self):
        """
        Installs index template and components
        """
        if not self.es_client:
            logger.error("Elasticsearch not connected for output setup!")
            raise ConnectionError("Elasticsearch not connected for output setup")

        setup_index(self.es_client, self.config["index"])

    def load(self) -> None:

        if self.do_setup is True:
            self.setup()

        while not self.done and not self.quit_when_idle:
            _count: int = 0
            _q: Generator[Any] = consume_queue(self.taskq)

            for _doc in _q:
                if self.done:
                    break
                if _doc is None:
                    if self.quit_when_idle:
                        break
                    continue

                _count += 1

                yield _doc

            _q.close()
            logger.info(f"Loaded {_count} documents in this batch")

    def run(self) -> None:
        def chunk(chunk_size=500):
            block = []
            for entry in self.load():
                if len(block) > chunk_size:

                    yield block
                    block = [entry]
                else:
                    block.append(entry)

            if block:
                yield block

        # _es_docs, _con_docs = tee(self.load, 2)
        self._setup_io()

        if self.es_client:
            for _chunk in chunk():
                for ok, action in helpers.streaming_bulk(
                    client=self.es_client,
                    index=self.config["index"],
                    actions=_chunk,
                ):
                    pass

        logger.debug("Shutting down output")

        # Cleanup
        if self.es_client is not None:
            logger.debug("Closing Elasticsearch output client.")
            self.es_client.close()

        self.done.set()
