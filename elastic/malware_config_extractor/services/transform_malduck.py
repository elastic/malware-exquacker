"""
Provides filter plugin to process event through malduck
"""
import io
import json
import logging
import random
import string
import sys
from dataclasses import dataclass
from functools import reduce
from pathlib import Path
from typing import Any, Dict, Generator, Iterable, List, Optional

import typer
from confluent_kafka import Consumer, KafkaException, Message, Producer, cimpl
from malduck.extractor import ExtractManager, ExtractorModules
from malduck.extractor.loaders import load_modules
from malduck.procmem import ProcessMemory
from scalpl import Cut

from elastic.malware_config_extractor import config

logger = logging.getLogger(__name__)
# logger.setLevel(logging.DEBUG)

BULK_NUM_MSG = 100
WORKING_FIELDNAME = "@metadata.malduck.temp"


def merge_dict(destination: Dict[str, Any], source: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recursively merge source into destination using the Dict API (also works for scalpl.Cut)
    >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } }
    >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } }
    >>> merge(a, b) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } }
    True
    """
    for key, value in source.items():
        if isinstance(value, dict):
            # get node or create one
            node = destination.setdefault(key, {})
            merge_dict(node, value)
        elif isinstance(value, list) and isinstance(destination.get(key), list):
            # Merge lists and dedupe values
            for item in value:
                if item not in destination[key]:
                    destination[key].append(item)
        else:
            destination[key] = value

    return destination


@dataclass(kw_only=True)
class MalduckTransform(Generator):
    """
    Consumes an input generator that should produce python dict documents.
    The field named in `field` is processed as bytes
    Returns None


    >>> events: List[Dict[str,Any]] = {"malware": }
    >>> malduck = MalduckTransform(input=events, field="bytes")
    >>> for event in malduck:
    >>>     print(event)

    """

    name: str = "malduck"
    id: Optional[str] = None  # pylint: disable=invalid-name
    input: Iterable[Dict[str, Any]]
    field: str
    target: Optional[str] = "threat.software.config"
    tags: Optional[List[str]] = None

    modules: Path

    def __post_init__(self):
        """Auto-generate unique ID if not specified and ensure modules path exists"""
        if self.id is None:
            chars = string.ascii_letters + string.digits
            self.id = "".join(random.choice(chars) for i in range(12))

        self._modules: ExtractorModules

        self.modules = Path(self.modules)
        if self.modules.exists():
            # Preload. This is a workaround until the utils module can be merged upstream
            load_modules(str(self.modules))
            self._modules = ExtractorModules(modules_path=str(self.modules))
        else:
            raise ValueError(f"Malduck modules path {self.modules} does not exist!")

        logging.debug("MalduckTransform state post-init: %s", self.__dict__)

    def throw(self, typ, val=None, tb=None):
        """Mandatory abstract implementation"""
        # pylint: disable=useless-super-delegation
        super().throw(typ, val, tb)

    def validate(self) -> bool:
        """Check configuration, assume we can write to stdout"""

        _valid = True
        if not self.modules.exists():
            logger.error("Malduck modules path '%s' does not exist", self.modules)
            _valid = False
        else:
            logger.debug("Malduck modules found at path: %s", self.modules)
            _manager = ExtractManager(self._modules)

            if not _manager.modules:
                logger.error("Malduck failed to load modules at path %s. Check dependencies.", self.modules)
                _valid = False
            else:
                logger.debug("Modules loaded: %s", _manager.modules.extractors)

        logging.debug("validate(): Everything checks out")

        return _valid

    def send(self, _ignored) -> Dict[str, Any] | None:
        """
        Process input iterable using value of `field` as reference to `bytes` to analyze.
        Extracted configuration will be placed at `target`
        """

        # Start with a clean manager - TODO: This might not be necessary
        logging.debug("Loading malduck modules: %s", self._modules)
        _manager = ExtractManager(self._modules)
        if not _manager.extractors:
            logger.error("No extractor modules found in Malduck modules path: %s", self.modules)

        event: Cut = next(self.input)

        _data = event.get(self.field, None)
        if _data is None:
            logger.warning("Field '%s' doesn't exist in event!", self.field)
            return event

        _manager.push_procmem(ProcessMemory(_data), rip_binaries=True)
        if _manager.config:
            # Merge configs first
            _merged: Dict[str, Any] = reduce(merge_dict, _manager.config, {})

            # Now format config into event
            event.setdefault(self.target, _merged)

        del event[self.field]

        return event


cli = typer.Typer(
    add_completion=False,
)


def kafka_consume(
    consumer: Consumer, topics: List[str], exit_on_eof: bool = False
) -> Generator[None, Dict[str, Any], None]:

    _assigned: bool = False
    _msgs: List[Message] = []
    msg: Optional[Message]

    # pylint: disable=c-extension-no-member
    def on_assignment(_c: cimpl.Consumer, _p: List[cimpl.TopicPartition]):
        _assigned = True

    consumer.subscribe(topics=topics, on_assign=on_assignment)

    while not _assigned:
        msg = consumer.poll(timeout=1.0)
        if msg is not None:
            if msg.error():
                raise KafkaException(msg.error())
        else:
            # If we've received a valid Message, then _assigned will be True
            _msgs.append(msg)

    while True:
        _batch: List[Message] = consumer.consume(BULK_NUM_MSG, 1.0)
        if len(_batch) == 0:
            continue
        else:
            _msgs.extend(_batch)

        while len(_msgs) > 0:
            msg = _msgs.pop()
            if msg.error():
                raise KafkaException(msg.error())

            _event: Dict[str, Any]
            _value: str = msg.value()
            try:
                _event = json.loads(_value)
            except json.JSONDecodeError:
                logger.error("Unable to parse event as JSON:\n%s", _value)
                _event = {"message": _value}

            event = Cut(_event)

            yield event


MAX_CHUNK_SIZE: int = 4096


def chunk_inflator(stream: io.BytesIO) -> bytes:
    import zlib

    _inflator = zlib.decompressobj()
    while True:  # Loop until EOF
        _chunk = stream.read(MAX_CHUNK_SIZE)
        if not _chunk:  # an empty string is the end
            yield _inflator.flush()
            break
        yield _inflator.decompress(_chunk)


def decode_decompress(input_gen: Iterable[Dict[str, Any]], field: str):
    from base64 import b64decode

    for item in input_gen:
        value = item.get(field)
        if value is None:
            continue

        blob_in: io.BytesIO = io.BytesIO(b64decode(value))
        blob_out: io.BytesIO = io.BytesIO()

        for chunk in chunk_inflator(blob_in):
            blob_out.write(chunk)

        item.setdefault(WORKING_FIELDNAME, blob_out.getvalue())

        yield item


@cli.command()
def transform(
    field: str = typer.Option(..., "--field", "-f", help="JSON path to field to be analyzed"),
    target: str = typer.Option("threat.software.config", "--target", "-t", help="JSON path to field to write output"),
    bootstrap_servers: List[str] = typer.Option(
        ..., "--bootstrap-servers", "-b", help="Bootstrap broker (host[:port])"
    ),
    modules: Path = typer.Option(..., help="Path to malduck modules directory"),
    input_topics: List[str] = typer.Option([config.QUACKABLE_ALERTS_TOPIC], "--input", "-i", help="Input Kafka topic"),
    output_topic: str = typer.Option(config.ENRICHED_ALERTS_TOPIC, "--output", "-o", help="Output Kafka topic"),
    _flag: bool = typer.Option(None, "--version", help="Show version information and exit.", is_eager=True),
):
    """
    Consumes a kafka topic, transforms using malduck, writes enriched events to kafka topic.
    """
    # pylint: disable=import-outside-toplevel
    from importlib.metadata import version

    if _flag:
        _version = version("elastic.malware_config_extractor")
        typer.echo(f"malware-exquacker cli version: {_version}")
        raise typer.Exit()

    _consumer = Consumer(
        {
            "bootstrap.servers": ",".join(bootstrap_servers),
            "group.id": "malware-exquacker",
            "auto.offset.reset": "earliest",
        }
    )

    _producer = Producer(
        {
            "bootstrap.servers": ",".join(bootstrap_servers),
        }
    )

    meta = None
    while meta is None:
        meta = _consumer.list_topics()

    k_gen = kafka_consume(consumer=_consumer, topics=input_topics)
    z_gen = decode_decompress(k_gen, field=field)
    m_gen = MalduckTransform(input=z_gen, field=WORKING_FIELDNAME, target=target, modules=modules)
    if not m_gen.validate():
        print("Invalid Malduck config!")
        return -1

    def delivery_report(err, msg):
        """Called once for each message produced to indicate delivery result.
        Triggered by poll() or flush()."""
        if err is not None:
            print(f"Message delivery failed: {err}")
        else:
            print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

    while True:
        for doc in m_gen:
            if "@metadata" in doc:
                del doc["@metadata"]

            # Trigger any available delivery report callbacks from previous produce() calls
            _producer.poll(0)

            # Asynchronously produce a message. The delivery report callback will
            # be triggered from the call to poll() above, or flush() below, when the
            # message has been successfully delivered or failed permanently.
            _producer.produce(
                topic=output_topic,
                value=json.dumps(doc.data),
                callback=delivery_report,
            )

        # Wait for any outstanding messages to be delivered and delivery report
        # callbacks to be triggered.
        _producer.flush()


def main():  # pylint: disable=missing-function-docstring
    cli(prog_name="transform-malduck")


if __name__ == "__main__":
    main()
